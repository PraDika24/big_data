{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d909e70a",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "#### 1. Menfilter Hanya Komentar Berbahasa Inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7ad1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder penyimpanan siap: filter_bahasa_inggris\n"
     ]
    }
   ],
   "source": [
    "#Buat folder output filter_bahasa_inggris jika belum ada\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = \"filter_bahasa_inggris\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Folder penyimpanan siap: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883467d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak dari koleksi: video_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa22a52346bf434da5bf23a8d8c14cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_1: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_1.csv (4520 komentar)\n",
      "Mengekstrak dari koleksi: video_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90789125d779441b8720793c65efac40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_2: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_2.csv (2812 komentar)\n",
      "Mengekstrak dari koleksi: video_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67796fdbce5e4395bddb648408e73e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_3: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_3.csv (5134 komentar)\n",
      "Mengekstrak dari koleksi: video_4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f179c5c2174a32b3cd4c2dfd6d58ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_4: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_4.csv (3391 komentar)\n",
      "Mengekstrak dari koleksi: video_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1919f58ee06b49e9bc6f3fd1a2399977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_5: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_5.csv (4087 komentar)\n",
      "Mengekstrak dari koleksi: video_6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cf72cfaca7412c9a7fc9497c430166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_6: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_6.csv (4089 komentar)\n",
      "Mengekstrak dari koleksi: video_7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5251a8da78b46af9d761b9e3f840620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_7: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_7.csv (3488 komentar)\n",
      "Mengekstrak dari koleksi: video_8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014cded191b347c8a4466d3e982b5dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Memfilter komentar di video_8: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: filter_bahasa_inggris\\video_8.csv (4819 komentar)\n"
     ]
    }
   ],
   "source": [
    "# Ekstrak & filter komentar dari MongoDB\n",
    "\n",
    "import sys\n",
    "\n",
    "# Tambahkan path parent folder ke sys.path supaya bisa impor collect_data.connection.py nya\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import csv\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "from tqdm.notebook import tqdm\n",
    "from collect_data.connection import get_db\n",
    "\n",
    "# Supaya hasil deteksi bahasa konsisten\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Fungsi deteksi bahasa, default ke Inggris jika tidak terdeteksi\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except LangDetectException:\n",
    "        # Jika tidak bisa deteksi, anggap sebagai bahasa Inggris\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Fungsi ekstraksi komentar dari satu koleksi\n",
    "def extract_english_comments(collection_name):\n",
    "    print(f\"Mengekstrak dari koleksi: {collection_name}\")\n",
    "    db = get_db(\"db_data_kotor\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    english_comments = []\n",
    "    cursor = collection.find()\n",
    "\n",
    "    # Mengambil Data Dokument pada Collection nya\n",
    "    for doc in tqdm(cursor, desc=f\"Memfilter komentar di {collection_name}\"):\n",
    "        try:\n",
    "            comment_data = doc[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comment_text = comment_data[\"textDisplay\"]\n",
    "            if is_english(comment_text):\n",
    "                english_comments.append({\n",
    "                    \"video_id\": doc[\"snippet\"][\"videoId\"],\n",
    "                    \"author_name\": comment_data[\"authorDisplayName\"],\n",
    "                    \"comment_text\": comment_text,\n",
    "                    \"published_at\": comment_data[\"publishedAt\"],\n",
    "                    \"updated_at\": comment_data[\"updatedAt\"]\n",
    "                })\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return english_comments\n",
    "\n",
    "# Daftar koleksi \n",
    "collection_names = [\n",
    "    \"video_1\", \"video_2\", \"video_3\", \"video_4\",\n",
    "    \"video_5\", \"video_6\", \"video_7\", \"video_8\"\n",
    "]\n",
    "\n",
    "# Proses setiap koleksi dan simpan ke CSV terpisah\n",
    "for name in collection_names:\n",
    "    comments = extract_english_comments(name)\n",
    "\n",
    "    # jika ada komentarnya maka akan menulis file csv sesuai nama collectionnya\n",
    "    if comments:\n",
    "        csv_filename = os.path.join(output_dir, f\"{name}.csv\")\n",
    "        with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(comments)\n",
    "\n",
    "        print(f\"Disimpan: {csv_filename} ({len(comments)} komentar)\")\n",
    "    else:\n",
    "        print(f\"Tidak ada komentar Inggris pada: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520a7423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh 10 komentar pertama dari video_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@areyoufreaked13</td>\n",
       "      <td>5:54:02 🤣🤣🤣</td>\n",
       "      <td>2025-05-25T16:30:34Z</td>\n",
       "      <td>2025-05-25T16:30:34Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@jolsonomaha</td>\n",
       "      <td>Ishowspeed can I draw a Luffy for you</td>\n",
       "      <td>2025-05-25T14:28:43Z</td>\n",
       "      <td>2025-05-25T14:28:43Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Hoshinomykisah</td>\n",
       "      <td>2:39:54</td>\n",
       "      <td>2025-05-23T21:15:52Z</td>\n",
       "      <td>2025-05-23T21:15:52Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@astro-zeet4028</td>\n",
       "      <td>he still got that luffy cover</td>\n",
       "      <td>2025-05-21T11:17:19Z</td>\n",
       "      <td>2025-05-21T11:17:19Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@JENNIEKIMSWORLD</td>\n",
       "      <td>Koreans sparked a revolution a new movement a ...</td>\n",
       "      <td>2025-05-19T04:16:16Z</td>\n",
       "      <td>2025-05-19T04:16:16Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@noeldenis404</td>\n",
       "      <td>😅😶‍🌫️</td>\n",
       "      <td>2025-05-17T12:51:55Z</td>\n",
       "      <td>2025-05-17T12:51:55Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@noeldenis404</td>\n",
       "      <td>456</td>\n",
       "      <td>2025-05-17T12:50:51Z</td>\n",
       "      <td>2025-05-17T12:50:51Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@noeldenis404</td>\n",
       "      <td>991</td>\n",
       "      <td>2025-05-17T12:50:36Z</td>\n",
       "      <td>2025-05-17T12:50:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@noeldenis404</td>\n",
       "      <td>7-6@134533</td>\n",
       "      <td>2025-05-17T12:48:03Z</td>\n",
       "      <td>2025-05-17T12:48:03Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@richabiswakarma3137</td>\n",
       "      <td>Speed and Jackson wang together   I don't want...</td>\n",
       "      <td>2025-05-17T08:36:17Z</td>\n",
       "      <td>2025-05-17T08:36:17Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id           author_name  \\\n",
       "0  fK85SQzm0Z0      @areyoufreaked13   \n",
       "1  fK85SQzm0Z0          @jolsonomaha   \n",
       "2  fK85SQzm0Z0       @Hoshinomykisah   \n",
       "3  fK85SQzm0Z0       @astro-zeet4028   \n",
       "4  fK85SQzm0Z0      @JENNIEKIMSWORLD   \n",
       "5  fK85SQzm0Z0         @noeldenis404   \n",
       "6  fK85SQzm0Z0         @noeldenis404   \n",
       "7  fK85SQzm0Z0         @noeldenis404   \n",
       "8  fK85SQzm0Z0         @noeldenis404   \n",
       "9  fK85SQzm0Z0  @richabiswakarma3137   \n",
       "\n",
       "                                        comment_text          published_at  \\\n",
       "0                                        5:54:02 🤣🤣🤣  2025-05-25T16:30:34Z   \n",
       "1              Ishowspeed can I draw a Luffy for you  2025-05-25T14:28:43Z   \n",
       "2                                            2:39:54  2025-05-23T21:15:52Z   \n",
       "3                      he still got that luffy cover  2025-05-21T11:17:19Z   \n",
       "4  Koreans sparked a revolution a new movement a ...  2025-05-19T04:16:16Z   \n",
       "5                                              😅😶‍🌫️  2025-05-17T12:51:55Z   \n",
       "6                                                456  2025-05-17T12:50:51Z   \n",
       "7                                                991  2025-05-17T12:50:36Z   \n",
       "8                                         7-6@134533  2025-05-17T12:48:03Z   \n",
       "9  Speed and Jackson wang together   I don't want...  2025-05-17T08:36:17Z   \n",
       "\n",
       "             updated_at  \n",
       "0  2025-05-25T16:30:34Z  \n",
       "1  2025-05-25T14:28:43Z  \n",
       "2  2025-05-23T21:15:52Z  \n",
       "3  2025-05-21T11:17:19Z  \n",
       "4  2025-05-19T04:16:16Z  \n",
       "5  2025-05-17T12:51:55Z  \n",
       "6  2025-05-17T12:50:51Z  \n",
       "7  2025-05-17T12:50:36Z  \n",
       "8  2025-05-17T12:48:03Z  \n",
       "9  2025-05-17T08:36:17Z  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tampilkan 10 komentar pertama dari salah satu file\n",
    "sample_file = os.path.join(output_dir, f\"{collection_names[0]}.csv\")\n",
    "if os.path.exists(sample_file):\n",
    "    df = pd.read_csv(sample_file)\n",
    "    print(f\"Contoh 10 komentar pertama dari\", collection_names[0])\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"File contoh tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2185b",
   "metadata": {},
   "source": [
    "#### 2. Membersihkan Komentar dari Emoji, Tanda Baca, Angka,Menghapus Duplikasi pada Komentar, Normalisasi Tanggal, dan Komentar Kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses: video_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 4520/4520 [00:00<00:00, 23992.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_1.csv (4033 komentar)\n",
      "Memproses: video_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 2812/2812 [00:00<00:00, 16157.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_2.csv (2542 komentar)\n",
      "Memproses: video_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 5134/5134 [00:00<00:00, 18974.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_3.csv (4672 komentar)\n",
      "Memproses: video_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 3391/3391 [00:00<00:00, 18209.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_4.csv (3013 komentar)\n",
      "Memproses: video_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 4087/4087 [00:00<00:00, 17973.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_5.csv (3690 komentar)\n",
      "Memproses: video_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 4089/4089 [00:00<00:00, 14373.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_6.csv (3690 komentar)\n",
      "Memproses: video_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 3488/3488 [00:00<00:00, 17650.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_7.csv (3224 komentar)\n",
      "Memproses: video_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Membersihkan komentar: 100%|██████████| 4819/4819 [00:00<00:00, 16508.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: cleaned_english_comments\\video_8.csv (4461 komentar)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Folder input dan output\n",
    "input_folder = \"filter_bahasa_inggris\"\n",
    "output_folder = \"cleaned_english_comments\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Pola emoji custom\n",
    "CUSTOM_EMOJI_PATTERNS = [\n",
    "    r'\\b(?:smiling|face|eyes|tears|lol|clap|fire|heart|laugh|rolling|cry|joy|wow|omg|cool|grin|blush|wink|sad|angry|love|shock|sleep|zzz|party|thinking|Thicc|BenNo|BenYes|Gyatt|WellWell|Sewey2|Demon|GodisGood|Banana|Box|football|Gay|harold|LLL|pikatchu|sus|monkey|uno|TIMEOUT)[a-z]*\\b'\n",
    "]\n",
    "# Membuat Regex bedasarkan pola emoji\n",
    "emoji_regex = re.compile(\"|\".join(CUSTOM_EMOJI_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# Pola URL/link\n",
    "URL_PATTERN = re.compile(r'(https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|ly|io|co|me)\\b)', re.IGNORECASE)\n",
    "\n",
    "def clean_comment(text):\n",
    "    text = text.lower()\n",
    "    text = emoji_regex.sub('', text) #Hapus Emoji\n",
    "    text = re.sub(r'\\d+', '', text) #Hapus angka\n",
    "    text = re.sub(URL_PATTERN, '', text)  # Hapus URL\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) #Hapus Tanda Baca\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #Hapus spasi berlebihan\n",
    "    return text\n",
    "\n",
    "def normalize_date(date_str):\n",
    "    try:\n",
    "        return datetime.fromisoformat(date_str.replace(\"Z\", \"\")).strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        return date_str\n",
    "\n",
    "# Proses pembersihan\n",
    "# Proses Looping Untuk Membaca file csv\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        required_columns = {\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"}\n",
    "        if not required_columns.issubset(set(df.columns)):\n",
    "            print(f\"Kolom tidak lengkap di file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Memproses: {filename}\")\n",
    "        \n",
    "        # Proses Bersihkan komentar\n",
    "        df[\"comment_text\"] = [clean_comment(t) for t in tqdm(df[\"comment_text\"].astype(str), desc=\"Membersihkan komentar\")]\n",
    "\n",
    "        # Hapus komentar kosong\n",
    "        df = df[df[\"comment_text\"].str.strip() != \"\"]\n",
    "\n",
    "        # Hapus komentar duplikat berdasarkan user, komentar, dan video\n",
    "        df = df.drop_duplicates(subset=[\"video_id\", \"author_name\", \"comment_text\"])\n",
    "\n",
    "        # Normalisasi tanggal\n",
    "        df[\"published_at\"] = df[\"published_at\"].astype(str).apply(normalize_date)\n",
    "        df[\"updated_at\"] = df[\"updated_at\"].astype(str).apply(normalize_date)\n",
    "\n",
    "        # Simpan hasil\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Disimpan: {output_path} ({len(df)} komentar)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7816e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh 10 komentar dari: video_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@jolsonomaha</td>\n",
       "      <td>ishowspeed can i draw a luffy for you</td>\n",
       "      <td>2025-05-25</td>\n",
       "      <td>2025-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@astro-zeet4028</td>\n",
       "      <td>he still got that luffy cover</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>2025-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@JENNIEKIMSWORLD</td>\n",
       "      <td>koreans sparked a revolution a new movement a ...</td>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>2025-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@richabiswakarma3137</td>\n",
       "      <td>speed and jackson wang together i dont want an...</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2025-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@in_uruguay</td>\n",
       "      <td>translator is assssh</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>2025-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@JENNIEKIMSWORLD</td>\n",
       "      <td>koreans sparked a revolution a new movement a ...</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>2025-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Sam_A_Sam</td>\n",
       "      <td>this makes me really chinese people</td>\n",
       "      <td>2025-05-05</td>\n",
       "      <td>2025-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Sam_A_Sam</td>\n",
       "      <td>the basketball street guy was the most interes...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Lucky123-h5r</td>\n",
       "      <td>china tour the best thing that happened to ish...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@fallen_angel_CZ</td>\n",
       "      <td>i go to china every year and youre telling me ...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id           author_name  \\\n",
       "0  fK85SQzm0Z0          @jolsonomaha   \n",
       "1  fK85SQzm0Z0       @astro-zeet4028   \n",
       "2  fK85SQzm0Z0      @JENNIEKIMSWORLD   \n",
       "3  fK85SQzm0Z0  @richabiswakarma3137   \n",
       "4  fK85SQzm0Z0           @in_uruguay   \n",
       "5  fK85SQzm0Z0      @JENNIEKIMSWORLD   \n",
       "6  fK85SQzm0Z0            @Sam_A_Sam   \n",
       "7  fK85SQzm0Z0            @Sam_A_Sam   \n",
       "8  fK85SQzm0Z0         @Lucky123-h5r   \n",
       "9  fK85SQzm0Z0      @fallen_angel_CZ   \n",
       "\n",
       "                                        comment_text published_at  updated_at  \n",
       "0              ishowspeed can i draw a luffy for you   2025-05-25  2025-05-25  \n",
       "1                      he still got that luffy cover   2025-05-21  2025-05-21  \n",
       "2  koreans sparked a revolution a new movement a ...   2025-05-19  2025-05-19  \n",
       "3  speed and jackson wang together i dont want an...   2025-05-17  2025-05-17  \n",
       "4                               translator is assssh   2025-05-15  2025-05-15  \n",
       "5  koreans sparked a revolution a new movement a ...   2025-05-09  2025-05-09  \n",
       "6                this makes me really chinese people   2025-05-05  2025-05-05  \n",
       "7  the basketball street guy was the most interes...   2025-05-04  2025-05-04  \n",
       "8  china tour the best thing that happened to ish...   2025-05-04  2025-05-04  \n",
       "9  i go to china every year and youre telling me ...   2025-05-04  2025-05-04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Folder hasil\n",
    "output_folder = \"cleaned_english_comments\"\n",
    "\n",
    "# Ganti nama file sesuai yang ingin dilihat\n",
    "filename = \"video_1.csv\"  # misalnya file hasil bersih dari video_1\n",
    "file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Baca dan tampilkan 10 komentar\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Contoh 10 komentar dari: {filename}\")\n",
    "    display(df[[\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"]].head(10))\n",
    "else:\n",
    "    print(\"File tidak ditemukan:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50203a",
   "metadata": {},
   "source": [
    "#### 3. Menghapus Spam dan Komentar Kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses: video_1.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_1.csv (3815 komentar tersisa)\n",
      "Memproses: video_2.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_2.csv (2340 komentar tersisa)\n",
      "Memproses: video_3.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_3.csv (4119 komentar tersisa)\n",
      "Memproses: video_4.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_4.csv (2756 komentar tersisa)\n",
      "Memproses: video_5.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_5.csv (3364 komentar tersisa)\n",
      "Memproses: video_6.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_6.csv (3381 komentar tersisa)\n",
      "Memproses: video_7.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_7.csv (2975 komentar tersisa)\n",
      "Memproses: video_8.csv\n",
      "Disimpan: cleaned_no_spam_comments\\video_8.csv (3990 komentar tersisa)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder input dan output baru\n",
    "input_folder = \"cleaned_english_comments\"\n",
    "output_folder = \"cleaned_no_spam_comments\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Daftar kata kunci spam\n",
    "spam_keywords = {\n",
    "    \"subscribe\", \"visit my channel\", \"check my channel\", \"follow me\", \"watch my video\",\n",
    "    \"support me\", \"free money\", \"win prize\", \"get rich\", \"earn cash\", \"free gift\",\n",
    "    \"click here\", \"claim now\", \"link in bio\", \"see below\", \"more info\", \"check link\",\n",
    "    \"visit link\", \"pls like\", \"spam alert\", \"bot comment\", \"auto comment\", \"buy now\",\n",
    "    \"shop here\", \"deal today\", \"limited offer\", \"exclusive content\", \"giveaway\", \"promo code\",\n",
    "    \"fast cash\", \"big win\", \"you won\", \"click this\", \"don't miss\", \"make money\", \"cash app\",\n",
    "    \"bitcoin giveaway\", \"100% free\", \"dm me\", \"message me\", \"contact me\", \"act now\",\n",
    "    \"urgent offer\", \"join now\", \"download now\", \"hot girls\", \"xxx\", \"onlyfans\", \"telegram group\",\n",
    "    \"vip access\", \"earn bitcoin\", \"credit card\", \"loan approval\", \"investment opportunity\",\n",
    "    \"referral code\", \"deal\", \"100% legit\", \"check bio\", \"cheap price\",\n",
    "    \"lowest price\", \"guaranteed\", \"earn daily\", \"instantly rich\",\n",
    "    \"get followers\", \"boost followers\", \"click the link\", \"like back\", \"real\", \"legit\",\n",
    "    \"don't skip\", \"no scam\", \"real account\", \"free followers\", \"click below\", \"amazing offer\",\n",
    "    \"too good to miss\", \"sponsored post\", \"giveaway now\", \"investment tips\", \"buy crypto\",\n",
    "    \"free promo\", \"vip group\", \"early access\", \"tap the link\", \"unlock content\",\n",
    "    \"sign up now\", \"get verified\", \"sfs\", \"f4f\", \"l4l\"\n",
    "}\n",
    "\n",
    "# Membuat Kamus Spam menjadi regex\n",
    "spam_patterns = re.compile(\"|\".join(re.escape(word) for word in spam_keywords), re.IGNORECASE)\n",
    "\n",
    "# Fungsi cek spam\n",
    "def is_spam(text):\n",
    "    return bool(spam_patterns.search(text))\n",
    "\n",
    "# Proses Looping untuk membaca setiap file di folder input\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        # Jika comment textnya \n",
    "        if \"comment_text\" not in df.columns:\n",
    "            print(f\"Kolom 'comment_text' tidak ditemukan di {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Memproses: {filename}\")\n",
    "        \n",
    "        # Hapus komentar kosong\n",
    "        df = df.dropna(subset=[\"comment_text\"])\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].astype(str).str.strip()\n",
    "        df = df[df[\"comment_text\"] != \"\"]\n",
    "\n",
    "        # Hapus komentar spam\n",
    "        # hanya komentar yang bukan spam yang akan disimpan ke dalam \n",
    "        df = df[~df[\"comment_text\"].apply(is_spam)]\n",
    "\n",
    "        # Simpan hasil\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Disimpan: {output_path} ({len(df)} komentar tersisa)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa9224a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh 10 komentar dari: video_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@jolsonomaha</td>\n",
       "      <td>ishowspeed can i draw a luffy for you</td>\n",
       "      <td>2025-05-25</td>\n",
       "      <td>2025-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@astro-zeet4028</td>\n",
       "      <td>he still got that luffy cover</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>2025-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@JENNIEKIMSWORLD</td>\n",
       "      <td>koreans sparked a revolution a new movement a ...</td>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>2025-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@richabiswakarma3137</td>\n",
       "      <td>speed and jackson wang together i dont want an...</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2025-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@in_uruguay</td>\n",
       "      <td>translator is assssh</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>2025-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@JENNIEKIMSWORLD</td>\n",
       "      <td>koreans sparked a revolution a new movement a ...</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>2025-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Sam_A_Sam</td>\n",
       "      <td>the basketball street guy was the most interes...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Lucky123-h5r</td>\n",
       "      <td>china tour the best thing that happened to ish...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@fallen_angel_CZ</td>\n",
       "      <td>i go to china every year and youre telling me ...</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Nikofutbol</td>\n",
       "      <td>ishow speed</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id           author_name  \\\n",
       "0  fK85SQzm0Z0          @jolsonomaha   \n",
       "1  fK85SQzm0Z0       @astro-zeet4028   \n",
       "2  fK85SQzm0Z0      @JENNIEKIMSWORLD   \n",
       "3  fK85SQzm0Z0  @richabiswakarma3137   \n",
       "4  fK85SQzm0Z0           @in_uruguay   \n",
       "5  fK85SQzm0Z0      @JENNIEKIMSWORLD   \n",
       "6  fK85SQzm0Z0            @Sam_A_Sam   \n",
       "7  fK85SQzm0Z0         @Lucky123-h5r   \n",
       "8  fK85SQzm0Z0      @fallen_angel_CZ   \n",
       "9  fK85SQzm0Z0           @Nikofutbol   \n",
       "\n",
       "                                        comment_text published_at  updated_at  \n",
       "0              ishowspeed can i draw a luffy for you   2025-05-25  2025-05-25  \n",
       "1                      he still got that luffy cover   2025-05-21  2025-05-21  \n",
       "2  koreans sparked a revolution a new movement a ...   2025-05-19  2025-05-19  \n",
       "3  speed and jackson wang together i dont want an...   2025-05-17  2025-05-17  \n",
       "4                               translator is assssh   2025-05-15  2025-05-15  \n",
       "5  koreans sparked a revolution a new movement a ...   2025-05-09  2025-05-09  \n",
       "6  the basketball street guy was the most interes...   2025-05-04  2025-05-04  \n",
       "7  china tour the best thing that happened to ish...   2025-05-04  2025-05-04  \n",
       "8  i go to china every year and youre telling me ...   2025-05-04  2025-05-04  \n",
       "9                                        ishow speed   2025-05-04  2025-05-04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Folder hasil\n",
    "output_folder = \"cleaned_no_spam_comments\"\n",
    "\n",
    "# Ganti nama file sesuai yang ingin dilihat\n",
    "filename = \"video_1.csv\"  # misalnya file hasil bersih dari video_1\n",
    "file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Baca dan tampilkan 10 komentar\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Contoh 10 komentar dari: {filename}\")\n",
    "    display(df[[\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"]].head(10))\n",
    "else:\n",
    "    print(\"File tidak ditemukan:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c227c",
   "metadata": {},
   "source": [
    "#### 4. Menormalisasi Kata Kata Slank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58a0844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses file: video_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 3815/3815 [00:00<00:00, 133001.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_1.csv\n",
      "Memproses file: video_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 2340/2340 [00:00<00:00, 122392.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_2.csv\n",
      "Memproses file: video_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 4119/4119 [00:00<00:00, 150446.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_3.csv\n",
      "Memproses file: video_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 2756/2756 [00:00<00:00, 142101.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_4.csv\n",
      "Memproses file: video_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 3364/3364 [00:00<00:00, 157607.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_5.csv\n",
      "Memproses file: video_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 3381/3381 [00:00<00:00, 148323.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_6.csv\n",
      "Memproses file: video_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 2975/2975 [00:00<00:00, 154902.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_7.csv\n",
      "Memproses file: video_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalisasi slang: 100%|██████████| 3990/3990 [00:00<00:00, 141410.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan ke: normalized_comments\\video_8.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Folder input dan output\n",
    "input_folder = \"cleaned_no_spam_comments\"\n",
    "output_folder = \"normalized_comments\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Kamus slang satu huruf\n",
    "slang_dict = {\n",
    "    \"lol\": \"laughing\",\n",
    "    \"lmao\": \"laughing\",\n",
    "    \"rofl\": \"laughing\",\n",
    "    \"brb\": \"returning\",\n",
    "    \"idk\": \"unknown\",\n",
    "    \"omg\": \"surprised\",\n",
    "    \"smh\": \"disappointed\",\n",
    "    \"btw\": \"anyway\",\n",
    "    \"tbh\": \"honestly\",\n",
    "    \"imo\": \"opinion\",\n",
    "    \"imho\": \"opinion\",\n",
    "    \"fyi\": \"information\",\n",
    "    \"asap\": \"quickly\",\n",
    "    \"bff\": \"friend\",\n",
    "    \"np\": \"ok\",\n",
    "    \"ttyl\": \"later\",\n",
    "    \"ikr\": \"agreed\",\n",
    "    \"afk\": \"away\",\n",
    "    \"gg\": \"good\",\n",
    "    \"idc\": \"indifferent\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thanks\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"y\": \"why\",\n",
    "    \"nvm\": \"nevermind\",\n",
    "    \"ootd\": \"outfit\",\n",
    "    \"pov\": \"perspective\",\n",
    "    \"fomo\": \"anxiety\",\n",
    "    \"goat\": \"best\",\n",
    "    \"fr\": \"real\",\n",
    "    \"ngl\": \"honestly\",\n",
    "    \"hmu\": \"contact\",\n",
    "    \"irl\": \"real\",\n",
    "    \"tmi\": \"excessive\",\n",
    "    \"wbu\": \"you\",\n",
    "    \"wyd\": \"doing\",\n",
    "    \"hbd\": \"birthday\",\n",
    "    \"g2g\": \"go\",\n",
    "    \"msg\": \"message\",\n",
    "    \"plz\": \"please\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"xoxo\": \"kisses\",\n",
    "    \"yolo\": \"live\",\n",
    "    \"rn\": \"now\",\n",
    "    \"idgaf\": \"indifferent\",\n",
    "    \"istg\": \"swear\",\n",
    "    \"ftw\": \"win\",\n",
    "    \"tbf\": \"fairly\",\n",
    "    \"otp\": \"pairing\",\n",
    "    \"rt\": \"retweet\",\n",
    "    \"dm\": \"message\",\n",
    "    \"pm\": \"message\",\n",
    "    \"vc\": \"video\",\n",
    "    \"wip\": \"progress\",\n",
    "    \"w\": \"good\", \n",
    "    \"slay\": \"excellent\",\n",
    "    \"simp\": \"flatterer\",\n",
    "    \"ghosting\": \"abandoning\",\n",
    "    \"flex\": \"boasting\",\n",
    "    \"sus\": \"suspicious\",\n",
    "    \"bet\": \"agreed\",\n",
    "    \"lit\": \"amazing\",\n",
    "    \"salty\": \"annoyed\",\n",
    "    \"cringe\": \"awkward\",\n",
    "    \"drip\": \"stylish\",\n",
    "    \"cap\": \"lie\",\n",
    "    \"lowkey\": \"quietly\",\n",
    "    \"highkey\": \"openly\",\n",
    "    \"yeet\": \"throw\",\n",
    "    \"rizz\": \"charm\",\n",
    "    \"bussin'\": \"delicious\",\n",
    "    \"gucci\": \"good\",\n",
    "    \"cheugy\": \"outdated\",\n",
    "    \"delulu\": \"delusional\",\n",
    "    \"iykyk\": \"understood\",\n",
    "    \"mood\": \"relatable\",\n",
    "    \"sheesh\": \"wow\",\n",
    "    \"periodt\": \"final\",\n",
    "    \"stan\": \"fanatic\",\n",
    "    \"squad\": \"group\",\n",
    "    \"fam\": \"friends\",\n",
    "    \"troll\": \"provoke\",\n",
    "    \"spam\": \"flood\",\n",
    "    \"pwned\": \"defeated\",\n",
    "    \"noob\": \"novice\",\n",
    "    \"roflmao\": \"laughing\",\n",
    "    \"ftfy\": \"fixed\",\n",
    "    \"icymi\": \"missed\",\n",
    "    \"tldr\": \"summary\",\n",
    "    \"wfh\": \"remote\",\n",
    "    \"af\": \"very\",\n",
    "    \"chad\": \"confident\",\n",
    "    \"cringey\": \"awkward\",\n",
    "    \"savage\": \"impressive\",\n",
    "    \"based\": \"opinionated\",\n",
    "    \"boujee\": \"luxurious\",\n",
    "    \"woke\": \"aware\",\n",
    "    \"mid\": \"average\",\n",
    "    \"tea\": \"gossip\",\n",
    "    \"dope\": \"cool\",\n",
    "    \"ain't\": \"isn't\",\n",
    "    \"gonna\": \"going\",\n",
    "    \"wanna\": \"want\",\n",
    "    \"gotta\": \"must\",\n",
    "    \"lemme\": \"let\",\n",
    "    \"gimme\": \"give\",\n",
    "    \"kinda\": \"somewhat\",\n",
    "    \"sorta\": \"somewhat\",\n",
    "    \"dunno\": \"unknown\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thru\": \"through\",\n",
    "    \"nite\": \"night\",\n",
    "    \"pls\": \"please\",\n",
    "    \"txt\": \"text\",\n",
    "    \"ur\": \"your\",\n",
    "    \"bc\": \"because\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"jk\": \"joking\",\n",
    "    \"k\": \"ok\",\n",
    "    \"kk\": \"ok\",\n",
    "    \"wby\": \"you\",\n",
    "    \"wtf\": \"shocked\",\n",
    "    \"2nite\": \"tonight\",\n",
    "    \"4ever\": \"forever\",\n",
    "    \"b4\": \"before\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"gud\": \"good\",\n",
    "    \"luv\": \"love\",\n",
    "    \"ne\": \"any\",\n",
    "    \"rly\": \"really\",\n",
    "    \"sum1\": \"someone\",\n",
    "    \"u're\": \"you\",\n",
    "    \"y?\": \"why\",\n",
    "    \"yep\": \"yes\",\n",
    "    \"nope\": \"no\",\n",
    "    \"xd\": \"funny\"\n",
    "}\n",
    "\n",
    "def normalize_text(text, slang_dict):\n",
    "    \"\"\"\n",
    "    Normalizes text by converting to lowercase, removing punctuation,\n",
    "    and replacing slang words based on the provided dictionary.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to normalize.\n",
    "        slang_dict (dict): A dictionary mapping slang words to their normalized forms.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Remove punctuation using regex, keeping only word characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Split text into words, ensuring proper word boundaries\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    # Replace words with their normalized forms if found in slang_dict, otherwise keep original word\n",
    "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
    "    # Join the normalized words back into a single string\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Proses semua file di folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal membaca file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"comment_text\" not in df.columns:\n",
    "            print(f\"Kolom 'comment_text' tidak ditemukan di file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Memproses file: {filename}\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].astype(str)\n",
    "        # Menggunakan fungsi normalize_text yang benar dan meneruskan slang_dict\n",
    "        # Kolom 'comment_text' sekarang akan langsung diperbarui dengan teks yang dinormalisasi\n",
    "        df[\"comment_text\"] = [\n",
    "            normalize_text(text, slang_dict) for text in tqdm(df[\"comment_text\"], desc=\"Normalisasi slang\")\n",
    "        ]\n",
    "\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        try:\n",
    "            df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "            print(f\"Disimpan ke: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal menyimpan file {output_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98817a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Contoh 10 komentar dari: video_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3805</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@IyKash</td>\n",
       "      <td>aint no way a greenapple on his shirt good str...</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3806</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@KINDLY7</td>\n",
       "      <td>good longest real stream</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@kaicenatleftcheek11</td>\n",
       "      <td>good stream even though speed in jail right now</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Cartiverynice</td>\n",
       "      <td>jail stream when</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3809</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@cristiannegrete8127</td>\n",
       "      <td>he got a green apple shirt now</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3810</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Fusion-v1</td>\n",
       "      <td>i was here</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@kiilee5963</td>\n",
       "      <td>one of the best streams good china</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@jcpesa19</td>\n",
       "      <td>henry turn the music off speed still vibing to...</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3813</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@lcfcalvin</td>\n",
       "      <td>now you dont know why this comment has so many...</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@5jqs</td>\n",
       "      <td>congrats on m good speed good hour china stream</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id           author_name  \\\n",
       "3805  fK85SQzm0Z0               @IyKash   \n",
       "3806  fK85SQzm0Z0              @KINDLY7   \n",
       "3807  fK85SQzm0Z0  @kaicenatleftcheek11   \n",
       "3808  fK85SQzm0Z0        @Cartiverynice   \n",
       "3809  fK85SQzm0Z0  @cristiannegrete8127   \n",
       "3810  fK85SQzm0Z0            @Fusion-v1   \n",
       "3811  fK85SQzm0Z0           @kiilee5963   \n",
       "3812  fK85SQzm0Z0             @jcpesa19   \n",
       "3813  fK85SQzm0Z0            @lcfcalvin   \n",
       "3814  fK85SQzm0Z0                 @5jqs   \n",
       "\n",
       "                                           comment_text published_at  \\\n",
       "3805  aint no way a greenapple on his shirt good str...   2025-03-24   \n",
       "3806                           good longest real stream   2025-03-24   \n",
       "3807    good stream even though speed in jail right now   2025-03-24   \n",
       "3808                                   jail stream when   2025-03-24   \n",
       "3809                     he got a green apple shirt now   2025-03-24   \n",
       "3810                                         i was here   2025-03-24   \n",
       "3811                 one of the best streams good china   2025-03-24   \n",
       "3812  henry turn the music off speed still vibing to...   2025-03-24   \n",
       "3813  now you dont know why this comment has so many...   2025-03-24   \n",
       "3814    congrats on m good speed good hour china stream   2025-03-24   \n",
       "\n",
       "      updated_at  \n",
       "3805  2025-03-24  \n",
       "3806  2025-03-24  \n",
       "3807  2025-03-24  \n",
       "3808  2025-03-24  \n",
       "3809  2025-03-24  \n",
       "3810  2025-03-24  \n",
       "3811  2025-03-24  \n",
       "3812  2025-03-24  \n",
       "3813  2025-04-16  \n",
       "3814  2025-03-24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folder hasil\n",
    "output_folder = \"normalized_comments\"\n",
    "\n",
    "# Ganti nama file sesuai yang ingin dilihat\n",
    "filename = \"video_1.csv\"  # misalnya file hasil bersih dari video_1\n",
    "file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Baca dan tampilkan 10 komentar\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\" Contoh 10 komentar dari: {filename}\")\n",
    "    display(df[[\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"]].tail(10))\n",
    "else:\n",
    "    print(\"File tidak ditemukan:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3662eb2",
   "metadata": {},
   "source": [
    "#### 5. Menghapus Kata Kata Non Bahasa Inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "207e674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Agus\n",
      "[nltk_data]     Handika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses: video_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 3815/3815 [00:00<00:00, 174465.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_1.csv (3759 komentar bersih)\n",
      "Memproses: video_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 2340/2340 [00:00<00:00, 148443.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_2.csv (2309 komentar bersih)\n",
      "Memproses: video_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 4119/4119 [00:00<00:00, 156729.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_3.csv (4044 komentar bersih)\n",
      "Memproses: video_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 2756/2756 [00:00<00:00, 147314.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_4.csv (2720 komentar bersih)\n",
      "Memproses: video_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 3364/3364 [00:00<00:00, 115856.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_5.csv (3324 komentar bersih)\n",
      "Memproses: video_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 3381/3381 [00:00<00:00, 144712.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_6.csv (3341 komentar bersih)\n",
      "Memproses: video_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 2975/2975 [00:00<00:00, 167609.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_7.csv (2932 komentar bersih)\n",
      "Memproses: video_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memfilter kata Inggris dari campuran: 100%|██████████| 3990/3990 [00:00<00:00, 162279.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disimpan: english_only_comments\\video_8.csv (3943 komentar bersih)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download kamus sekali saja jika belum\n",
    "nltk.download(\"words\")\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Folder input/output\n",
    "input_folder = \"normalized_comments\"\n",
    "output_folder = \"english_only_comments\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def keep_english_words(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text)  # Ambil kata alfabet\n",
    "    filtered = [word for word in tokens if word.lower() in english_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# Proses file CSV\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        if \"comment_text\" not in df.columns:\n",
    "            print(f\"Kolom 'comment_text' tidak ditemukan di {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Memproses: {filename}\")\n",
    "        tqdm.pandas(desc=\"Memfilter kata Inggris dari campuran\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].astype(str).progress_apply(keep_english_words)\n",
    "\n",
    "        # Hapus komentar kosong/NaN\n",
    "        df = df[df[\"comment_text\"].str.strip() != \"\"]\n",
    "        df = df.dropna(subset=[\"comment_text\"])\n",
    "\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Disimpan: {output_path} ({len(df)} komentar bersih)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58d4bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh 10 komentar dari: video_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@IyKash</td>\n",
       "      <td>aint no way a on his shirt good stream though ...</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@KINDLY7</td>\n",
       "      <td>good real stream</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@kaicenatleftcheek11</td>\n",
       "      <td>good stream even though speed in jail right now</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Cartiverynice</td>\n",
       "      <td>jail stream when</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@cristiannegrete8127</td>\n",
       "      <td>he got a green apple shirt now</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@Fusion-v1</td>\n",
       "      <td>i was here</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@kiilee5963</td>\n",
       "      <td>one of the best good china</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@jcpesa19</td>\n",
       "      <td>henry turn the music off speed still to nae na...</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@lcfcalvin</td>\n",
       "      <td>now you dont know why this comment so many</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>fK85SQzm0Z0</td>\n",
       "      <td>@5jqs</td>\n",
       "      <td>on m good speed good hour china stream</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id           author_name  \\\n",
       "3749  fK85SQzm0Z0               @IyKash   \n",
       "3750  fK85SQzm0Z0              @KINDLY7   \n",
       "3751  fK85SQzm0Z0  @kaicenatleftcheek11   \n",
       "3752  fK85SQzm0Z0        @Cartiverynice   \n",
       "3753  fK85SQzm0Z0  @cristiannegrete8127   \n",
       "3754  fK85SQzm0Z0            @Fusion-v1   \n",
       "3755  fK85SQzm0Z0           @kiilee5963   \n",
       "3756  fK85SQzm0Z0             @jcpesa19   \n",
       "3757  fK85SQzm0Z0            @lcfcalvin   \n",
       "3758  fK85SQzm0Z0                 @5jqs   \n",
       "\n",
       "                                           comment_text published_at  \\\n",
       "3749  aint no way a on his shirt good stream though ...   2025-03-24   \n",
       "3750                                   good real stream   2025-03-24   \n",
       "3751    good stream even though speed in jail right now   2025-03-24   \n",
       "3752                                   jail stream when   2025-03-24   \n",
       "3753                     he got a green apple shirt now   2025-03-24   \n",
       "3754                                         i was here   2025-03-24   \n",
       "3755                         one of the best good china   2025-03-24   \n",
       "3756  henry turn the music off speed still to nae na...   2025-03-24   \n",
       "3757         now you dont know why this comment so many   2025-03-24   \n",
       "3758             on m good speed good hour china stream   2025-03-24   \n",
       "\n",
       "      updated_at  \n",
       "3749  2025-03-24  \n",
       "3750  2025-03-24  \n",
       "3751  2025-03-24  \n",
       "3752  2025-03-24  \n",
       "3753  2025-03-24  \n",
       "3754  2025-03-24  \n",
       "3755  2025-03-24  \n",
       "3756  2025-03-24  \n",
       "3757  2025-04-16  \n",
       "3758  2025-03-24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folder hasil\n",
    "output_folder = \"english_only_comments\"\n",
    "\n",
    "# Ganti nama file sesuai yang ingin dilihat\n",
    "filename = \"video_1.csv\"  # misalnya file hasil bersih dari video_1\n",
    "file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Baca dan tampilkan 10 komentar\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Contoh 10 komentar dari: {filename}\")\n",
    "    display(df[[\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"]].tail(10))\n",
    "else:\n",
    "    print(\"File tidak ditemukan:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f2eb2",
   "metadata": {},
   "source": [
    "#### 6. Simpan Data ke MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6792601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses file: video_1.csv → collection: video_1\n",
      "Berhasil menyimpan 3759 data ke MongoDB.\n",
      "Memproses file: video_2.csv → collection: video_2\n",
      "Berhasil menyimpan 2309 data ke MongoDB.\n",
      "Memproses file: video_3.csv → collection: video_3\n",
      "Berhasil menyimpan 4044 data ke MongoDB.\n",
      "Memproses file: video_4.csv → collection: video_4\n",
      "Berhasil menyimpan 2720 data ke MongoDB.\n",
      "Memproses file: video_5.csv → collection: video_5\n",
      "Berhasil menyimpan 3324 data ke MongoDB.\n",
      "Memproses file: video_6.csv → collection: video_6\n",
      "Berhasil menyimpan 3341 data ke MongoDB.\n",
      "Memproses file: video_7.csv → collection: video_7\n",
      "Berhasil menyimpan 2932 data ke MongoDB.\n",
      "Memproses file: video_8.csv → collection: video_8\n",
      "Berhasil menyimpan 3943 data ke MongoDB.\n",
      "Semua file selesai diproses.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_folder = \"english_only_comments\"\n",
    "db = get_db(\"db_komentar_bersih\")\n",
    "\n",
    "def save_df_to_mongodb(df, collection):\n",
    "    records = df.to_dict(orient='records')\n",
    "    if records:\n",
    "        collection.insert_many(records)\n",
    "        print(f\"Berhasil menyimpan {len(records)} data ke MongoDB.\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        required_columns = {\"video_id\", \"author_name\", \"comment_text\", \"published_at\", \"updated_at\"}\n",
    "        if not required_columns.issubset(set(df.columns)):\n",
    "            print(f\"Kolom tidak lengkap di file: {filename}, lewati file ini.\")\n",
    "            continue\n",
    "\n",
    "        collection_name = os.path.splitext(filename)[0]\n",
    "        collection = db[collection_name]\n",
    "\n",
    "        print(f\"Memproses file: {filename} → collection: {collection_name}\")\n",
    "        save_df_to_mongodb(df, collection)\n",
    "\n",
    "print(\"Semua file selesai diproses.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36855d54",
   "metadata": {},
   "source": [
    "#### 6. Gabungkan Collection untuk Proses Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c134b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggabungkan data dari 8 collection di db sumber...\n",
      " - Mengambil 2720 dokumen dari collection 'video_4'\n",
      " - Mengambil 2309 dokumen dari collection 'video_2'\n",
      " - Mengambil 3324 dokumen dari collection 'video_5'\n",
      " - Mengambil 3943 dokumen dari collection 'video_8'\n",
      " - Mengambil 3341 dokumen dari collection 'video_6'\n",
      " - Mengambil 3759 dokumen dari collection 'video_1'\n",
      " - Mengambil 4044 dokumen dari collection 'video_3'\n",
      " - Mengambil 2932 dokumen dari collection 'video_7'\n",
      "Berhasil menggabungkan total 26372 dokumen ke collection 'merge_data' di db merge\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def merge_collections_between_dbs(\n",
    "    db_source, db_merge, \n",
    "    target_collection_name=\"merge_data\"\n",
    "):\n",
    "    # Hapus collection target di db_merge jika sudah ada\n",
    "    if target_collection_name in db_merge.list_collection_names():\n",
    "        db_merge[target_collection_name].drop()\n",
    "        print(f\"Collection '{target_collection_name}' di db merge dihapus dulu agar fresh.\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Ambil semua collection di db_source\n",
    "    collections = db_source.list_collection_names()\n",
    "    print(f\"Menggabungkan data dari {len(collections)} collection di db sumber...\")\n",
    "\n",
    "    for col_name in collections:\n",
    "        collection = db_source[col_name]\n",
    "        docs = list(collection.find())\n",
    "        print(f\" - Mengambil {len(docs)} dokumen dari collection '{col_name}'\")\n",
    "        all_data.extend(docs)\n",
    "\n",
    "    if all_data:\n",
    "        # Hapus _id supaya tidak konflik saat insert di db_merge\n",
    "        for doc in all_data:\n",
    "            if \"_id\" in doc:\n",
    "                del doc[\"_id\"]\n",
    "\n",
    "        db_merge[target_collection_name].insert_many(all_data)\n",
    "        print(f\"Berhasil menggabungkan total {len(all_data)} dokumen ke collection '{target_collection_name}' di db merge\")\n",
    "    else:\n",
    "        print(\"Tidak ada data untuk digabungkan.\")\n",
    "\n",
    "# Contoh pemakaian:\n",
    "db_source = get_db(\"db_komentar_bersih\")   # ganti dengan nama db sumbermu\n",
    "db_merge = get_db(\"db_komentar_merge\")     # ganti dengan nama db merge (target)\n",
    "\n",
    "merge_collections_between_dbs(db_source, db_merge, target_collection_name=\"merge_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
